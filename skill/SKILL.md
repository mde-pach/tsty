---
name: tsty
description: "Autonomous E2E testing framework for visual QA and GitHub issue fixing. AUTONOMOUSLY FIXES GITHUB ISSUES: fetch ‚Üí create flow ‚Üí link ‚Üí run reference ‚Üí analyze screenshots ‚Üí fix code ‚Üí re-run ‚Üí compare visually. Use when asked to: (1) fix/resolve GitHub issues; (2) test/verify UI, frontend, web apps; (3) check layout, visual bugs, styling, design; (4) test accessibility/WCAG; (5) run E2E/integration/user flow tests; (6) test form submissions, interactions; (7) verify API integrations; (8) regression test after changes; (9) debug UI issues; (10) analyze screenshots, console logs, reports; (11) automate browser testing with Playwright. Works with ANY tech stack. CRITICAL: Analyzes screenshots visually (not just exit codes), compares before/after, iterates autonomously until visually verified. Uses micro-iteration and fail-fast mode."
---

# Tsty - Iterative Visual QA Testing Skill

**Autonomous E2E testing framework** that creates flows, runs tests, analyzes results, fixes issues, and re-runs until passing. **AUTONOMOUSLY FIXES GITHUB ISSUES** end-to-end: fetch ‚Üí create test ‚Üí link flow ‚Üí run reference ‚Üí analyze ‚Üí fix code ‚Üí re-run ‚Üí compare visually. Includes before/after screenshot comparison for visual verification.

## ‚ö†Ô∏è CRITICAL PRINCIPLES (Read These First)

**These are the MOST COMMON failure modes from real testing sessions. Violating these wastes 60+ minutes per session.**

### 1. üì∏ Screenshot Analysis - Strategic, Not Mechanical (MOST IMPORTANT)

**You are autonomous, but you MUST analyze screenshots visually WHEN NEEDED.**

```
Exit code 0 ‚â† Feature works
Screenshots show the truth
Visual evidence is PRIMARY source
BUT analyze strategically to save time & tokens
```

**‚ö†Ô∏è MANDATORY ANALYSIS (ALWAYS analyze):**

1. **Test FAILED** - Need to understand why
2. **First time seeing a page** - Need to understand layout
3. **Before committing a fix** - Visual verification required
4. **Visual bug investigation** - Layout, styling, design issues

**‚úÖ OPTIONAL ANALYSIS (Skip to save time):**

1. **Test PASSED + seen page before + no visual changes expected**
2. **Health checks** (unless failed)
3. **Intermediate navigation steps**
4. **Re-running same test without code changes**

**üìã Smart Analysis Process:**

**When analysis IS needed:**
1. List screenshots: `ls -1 .tsty/screenshots/run-<flow-id>-<timestamp>/*.png`
2. Read relevant PNGs (failure point or target page, not ALL if unnecessary)
3. Analyze what you see (2-3 sentences)
4. Document issues

**When analysis NOT needed:**
1. List screenshots (verify they exist)
2. Note: "Test passed, page previously verified, skipping detailed analysis"
3. Continue to next step

‚Üí **Full details: [SMART-SCREENSHOT-ANALYSIS.md](references/SMART-SCREENSHOT-ANALYSIS.md)**
‚Üí **Screenshot caching: [SCREENSHOT-CACHE.md](references/SCREENSHOT-CACHE.md)** (re-use descriptions, save 60-80% tokens on re-runs)

**Before committing ANY fix:**

1. **List screenshots from BOTH runs:**
   ```bash
   ls -1 .tsty/screenshots/run-<flow>-<before-timestamp>/*.png
   ls -1 .tsty/screenshots/run-<flow>-<after-timestamp>/*.png
   ```

2. **Read screenshots from BOTH runs** using the Read tool.

3. **Compare visually:**
   - What was wrong in BEFORE screenshots?
   - What changed in AFTER screenshots?
   - Is the issue visually fixed?

4. **Only commit if visual verification passes.**

**üö® SCREENSHOT ANALYSIS ENFORCEMENT üö®**

**YOU CANNOT SKIP SCREENSHOT ANALYSIS. This is a BLOCKING REQUIREMENT.**

**After EVERY test run, you MUST demonstrate you've analyzed screenshots by:**

1. **Listing them:** Show the `ls -1 .tsty/screenshots/...` output
2. **Reading them:** Use Read tool on EVERY PNG file
3. **Describing what you see:** Write 2-3 sentences about WHAT IS VISUALLY PRESENT in each screenshot
4. **Identifying issues:** List ANY visual problems (error pages, broken layouts, missing elements)

**Common mistake:** Saying "I'll analyze screenshots" without actually Reading the PNG files.

**Correct pattern:**
```
Test passed. Now analyzing screenshots:

1. Listing: ls -1 .tsty/screenshots/run-xxx-xxx/*.png
   Output: 1-homepage.png, 2-issue-page.png

2. Reading screenshot 1: Read .tsty/screenshots/.../1-homepage.png
   Visual observation: Shows Next.js error overlay with red "Runtime Error" banner.
   Text visible: "ENOENT: no such file or directory, open '/Users/.../pages/_document.js'"
   Conclusion: Server has build error, NOT working homepage.

3. Reading screenshot 2: Read .tsty/screenshots/.../2-issue-page.png
   Visual observation: Same error page as screenshot 1.
   Conclusion: Build error prevents all pages from loading.

4. Issue identified: Server running but has build errors. Must fix build before testing.
```

**DO NOT proceed to next step without completing this 4-part analysis.**

**‚ö†Ô∏è MANDATORY: Before proceeding with ANY test, read the complete workflow:**
```
Read skill/references/VISUAL-ANALYSIS-WORKFLOW.md
```
This contains the step-by-step process, examples of good vs bad analysis, and common mistakes to avoid.

### 2. üë§ Test Like a Human User (NOT test automation engineer)

**ALWAYS ask: "How would a human do this?" Then do exactly that.**

```json
‚úÖ CORRECT (user-like):
{ "type": "click", "selector": "text=Save" }
{ "type": "click", "selector": "text=Focus" }
{ "type": "fill", "selector": "placeholder=Email", "value": "test@test.com" }

‚ùå WRONG (engineer-like):
{ "type": "dragAndDrop", ... }  // Before trying click!
{ "type": "evaluate", "pageFunction": "..." }  // Before trying UI interaction!
{ "selector": "div:has-text('Focus'):has-text('Focus an element')" }  // Over-complex!
```

**Interaction hierarchy (try in order):**
1. Simple click (`text=`, `placeholder=`, `button:has-text()`)
2. Fill/type for inputs
3. Keyboard shortcuts (if user would use them)
4. Drag-and-drop (ONLY if click fails first)
5. JavaScript evaluate (LAST RESORT - usually means bug!)

**‚Üí Details: [USER-FIRST-TESTING.md](references/USER-FIRST-TESTING.md)**

### 3. üîß Fix Bugs, Don't Assume Framework Limitations

**Test passed ‚úì + Screenshot unchanged ‚úó = FALSE SUCCESS = App bug**

```
DEFAULT ASSUMPTION (from real data):
‚îú‚îÄ 70% Application bugs (missing onClick, broken logic)
‚îú‚îÄ 25% Wrong test selectors
‚îú‚îÄ 4% Timing issues
‚îî‚îÄ 1% Framework limitations

When feature doesn't work:
1. Try simple user interaction (text= + click)
2. Still broken? Read the component code
3. Find the bug (missing handler, wrong logic)
4. Fix the application code
5. Re-run test
6. Verify fix worked
```

**NEVER conclude "framework limitation" without:**
- ‚úÖ Trying simple `text=` selector + click
- ‚úÖ Reading component code
- ‚úÖ Verifying event handlers exist
- ‚úÖ Testing 3+ different approaches

**‚Üí Details: [BUG-FIXING-WORKFLOW.md](references/BUG-FIXING-WORKFLOW.md)**

### 4. üîÑ Autonomous Iteration (Don't Just Report)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ YOU ARE THE TESTER AND THE FIXER        ‚îÇ
‚îÇ Don't report failures - FIX them        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Loop: CREATE ‚Üí RUN ‚Üí ANALYZE ‚Üí FIX ‚Üí RE-RUN
Exit when: exit code 0 AND screenshots correct
```

**What to fix:**
- Console errors ‚Üí Fix **application JavaScript**
- Selector timeouts ‚Üí Fix **test selectors**
- Visual bugs ‚Üí Fix **application CSS/layout**
- Missing features ‚Üí **Add them to the app**

**‚Üí Details: [ITERATIVE-WORKFLOW.md](references/ITERATIVE-WORKFLOW.md)**

### 5. üß™ Micro-Iteration (Test ONE Action at a Time)

```
‚ùå NEVER:
1. Create 9 actions
2. Create 9-step flow
3. Run ‚Üí All fail
4. Waste 30+ min debugging

‚úÖ ALWAYS:
1. Create 1 action
2. Test in 2-step flow (15s)
3. Fix if fails
4. Create next action (only after #1 works)
5. Repeat
```

**The Iron Rule:** Never have more than 1 untested action.

**‚Üí Details: [ITERATIVE-WORKFLOW.md](references/ITERATIVE-WORKFLOW.md)**

### 6. üéØ Autonomous GitHub Issue Fixing is MANDATORY

**When user says "fix issue #X" or "handle GitHub issue" ‚Üí COMPLETE AUTONOMOUS WORKFLOW**

```
FULL WORKFLOW (ALL STEPS REQUIRED):
1. ‚úÖ Fetch issue from GitHub
2. ‚úÖ Store locally in .tsty/issues/
3. üÜï RUN PRE-FLIGHT CHECKS (QUICK - 10 seconds)
   a. Check server running (lsof or curl)
   b. Verify server health (curl + grep for errors)
   c. If errors found ‚Üí Fix BEFORE proceeding
   d. SKIP screenshot analysis unless errors detected
   ‚Üí **Optimized approach: [OPTIMIZED-PREFLIGHT.md](references/OPTIMIZED-PREFLIGHT.md)**
4. ‚úÖ AUTO-CREATE test flow (.tsty/flows/issue-{number}-{slug}.json)
5. ‚úÖ AUTO-LINK flow to issue (set linkedFlowId in issue JSON)
6. ‚úÖ AUTO-RUN and mark as reference (run + extract runId + set referenceRunId)
7. ‚úÖ AUTO-ANALYZE screenshots (strategically - see analysis rules above)
8. ‚úÖ Apply fixes based on visual analysis
9. ‚úÖ AUTO-RE-RUN (capture AFTER state)
10. ‚úÖ AUTO-COMPARE (read critical screenshots from BOTH runs, verify improvement)
```

**Do NOT stop at step 2!** The complete workflow is: fetch ‚Üí create ‚Üí link ‚Üí run ‚Üí analyze ‚Üí fix ‚Üí re-run ‚Üí compare.

**‚Üí Complete workflow: [AUTONOMOUS-ISSUE-FIXING.md](references/AUTONOMOUS-ISSUE-FIXING.md)**
**‚Üí Linking details: [ISSUE-FLOW-LINKING.md](references/ISSUE-FLOW-LINKING.md)**

---

## üö® OUTCOME VERIFICATION IS MANDATORY

**‚ö†Ô∏è CRITICAL: "Test passed" ‚â† "Feature works"**

```
Exit code 0 means: Playwright didn't crash
Feature works means: A human user can accomplish their goal

ALWAYS VERIFY BOTH
```

### After Test Runs (Strategic Analysis)

**Step 1: List all screenshots (ALWAYS)**

```bash
ls -1 .tsty/screenshots/<run-id>/*.png
```

**Step 2: Decide if analysis needed (use decision tree above)**

**If ANALYSIS NEEDED (failed, first time, before commit):**
```
Read critical screenshots (failure point or target page)
Analyze visually (2-3 sentences)
Document issues
```

**If ANALYSIS NOT NEEDED (passed, seen before, no changes):**
```
Note: "Screenshots listed, test passed, skipping detailed analysis"
Continue
```

**Step 3: Visual Verification Checklist (when analyzing)**

‚ñ° **Screenshot shows expected UI change?**
  - Button clicked ‚Üí Something appeared/changed?
  - Form filled ‚Üí Data visible in UI?
  - Item added ‚Üí Count increased?

‚ñ° **Data created/updated (check files)?**
  - File exists AND has correct content?
  - Database updated with right values?
  - API called with expected payload?

‚ñ° **Would a real user be satisfied?**
  - Feature actually usable?
  - No broken UI or partial states?
  - Workflow completes end-to-end?

**If ANY checkbox is unchecked ‚Üí BUG in application. Investigate immediately.**

**Step 4: Before committing fixes**

After applying fixes and re-running, list screenshots from both runs:
```bash
ls -1 .tsty/screenshots/<before-run-id>/*.png
ls -1 .tsty/screenshots/<after-run-id>/*.png
```

You MUST:
- Read screenshots from BOTH runs
- Compare visually (before vs after)
- Verify visual improvements are visible
- Only commit if screenshots show the fix worked

### Common False Positives (CRITICAL)

These scenarios report success but feature is broken:

```
‚ùå Click succeeded ‚Üí But onClick handler missing
   Test: ‚úÖ Step passed
   Reality: ‚ùå Nothing happened (screenshot unchanged)

‚ùå Fill succeeded ‚Üí But validation blocked save
   Test: ‚úÖ Form filled
   Reality: ‚ùå Data not saved (check file contents)

‚ùå Navigation succeeded ‚Üí But redirect didn't happen
   Test: ‚úÖ Navigated to /save
   Reality: ‚ùå Still on /edit (check URL/screenshot)

‚ùå Action completed ‚Üí But feature half-broken
   Test: ‚úÖ All steps passed
   Reality: ‚ùå UI shows error state (check screenshot)
```

**Default assumption: If screenshot shows no change ‚Üí Application bug, not test issue.**

**‚Üí Complete verification guide: [VERIFICATION-CHECKLIST.md](references/VERIFICATION-CHECKLIST.md)**

### üîí Screenshot Analysis Enforcement Checklist

**Before marking ANY test as complete or closing ANY issue, verify:**

- [ ] Listed all screenshots: `ls -1 .tsty/screenshots/<run-id>/*.png`
- [ ] Read EVERY screenshot PNG file (not just acknowledged they exist)
- [ ] Documented visual observations in detail (what UI elements, what state, what changed)
- [ ] For fixes: Compared before/after screenshots from both runs
- [ ] Verified changes are VISIBLE in screenshots (not just code changed or tests passed)

**If ANY checkbox is unchecked ‚Üí You skipped required steps. Go back and complete them.**

---

## üöÄ PRE-FLIGHT CHECKS (MANDATORY BEFORE TESTING)

**‚ö†Ô∏è BLOCKING REQUIREMENT: Run these checks BEFORE creating or running ANY test flow.**

### Why Pre-Flight Checks Are Critical

```
‚ùå Real failure: Test passed (exit 0) but screenshots showed Next.js error page
‚ùå Root cause: Server running but had build errors
‚ùå Time wasted: 60+ minutes
‚úÖ Solution: Pre-flight checks catch this in 30 seconds
```

### Quick Pre-Flight Checklist

**Before EVERY test run:**

1. **Check server is running:**
   ```bash
   lsof -i :4000 || curl -s http://localhost:4000 > /dev/null
   ```
   If not running: `tsty > /tmp/tsty-dashboard.log 2>&1 &`

2. **Verify server health (CRITICAL - catches build errors):**
   ```bash
   curl -s http://localhost:4000 | grep -q "Runtime Error\|Failed to compile\|ENOENT" && echo "‚ùå SERVER HAS ERRORS" || echo "‚úÖ SERVER OK"
   ```
   **If errors found:** Fix build/runtime errors BEFORE running tests

3. **Run health-check flow:**
   ```bash
   # Create minimal health check
   cat > .tsty/flows/_health-check.json << 'EOF'
   {
     "name": "Health Check",
     "baseUrl": "http://localhost:4000",
     "failFast": true,
     "monitorConsole": false,
     "playwright": { "headless": true, "timeout": 10000 },
     "steps": [{
       "name": "Homepage loads",
       "url": "/",
       "capture": { "screenshot": true },
       "primitives": [
         { "type": "waitForLoadState", "options": { "state": "networkidle" } }
       ]
     }]
   }
   EOF

   # Run it
   tsty run _health-check --fail-fast --no-monitor
   ```

4. **MANDATORY: Read health-check screenshot:**
   ```bash
   # List screenshot
   ls -1 .tsty/screenshots/run-_health-check-*/1-*.png | tail -1

   # Use Read tool on the screenshot
   # Verify it shows working homepage, NOT error page
   ```

**If health check shows error page ‚Üí STOP. Fix server/build first.**

**‚Üí Complete guide: [PRE-FLIGHT-CHECKS.md](references/PRE-FLIGHT-CHECKS.md)**

---

## Quick Start

1. **Initialize:** `tsty init`
2. **Create discovery flow** to capture HTML: [E2E-TESTING-GUIDE.md](references/E2E-TESTING-GUIDE.md)
3. **Extract selectors** from HTML (use exact `placeholder=`, `text=` from captured HTML)
4. **Create action** with simple selectors
5. **Test immediately:** `tsty run flow-name --fail-fast`
6. **Verify outcome:** Check screenshot shows expected change (not just exit code 0)
7. **If doesn't work:** Read code ‚Üí Fix bug ‚Üí Re-test

**‚Üí Full workflow: [E2E-TESTING-GUIDE.md](references/E2E-TESTING-GUIDE.md)**

---

## Flow Creation for GitHub Issues

**When creating a test flow for a GitHub issue, follow these patterns:**

### Naming Convention
- **Format:** `issue-{number}-{slug}.json`
- **Slug:** 2-4 words from issue title, kebab-case
- **Examples:**
  - Issue #42 "Fix checkout submit button" ‚Üí `issue-42-checkout-submit.json`
  - Issue #1 "Improve comparison layout" ‚Üí `issue-1-comparison-layout.json`

### Flow Structure Template

```json
{
  "name": "Issue #{number}: {title}",
  "description": "{issue description}",
  "baseUrl": "http://localhost:4000",  // Adjust for actual app
  "failFast": true,
  "monitorConsole": false,  // Usually false for dev servers
  "playwright": {
    "headless": true,
    "timeout": 30000
  },
  "steps": [
    {
      "name": "Navigate to affected page",
      "url": "/path/to/bug",
      "capture": { "screenshot": true },
      "primitives": [
        { "type": "waitForLoadState", "options": { "state": "networkidle" } },
        { "type": "waitForTimeout", "timeout": 2000 }
      ]
    },
    {
      "name": "Trigger the bug",
      "primitives": [
        { "type": "click", "selector": "text=Button" }
      ],
      "capture": { "screenshot": true }
    }
  ]
}
```

### Flow Creation Process

1. **Identify the URL** where the issue is visible
2. **Create minimal flow** that navigates to that URL
3. **Add interactions** to trigger the bug (if needed)
4. **Capture screenshots** at each step
5. **Save to** `.tsty/flows/issue-{number}-{slug}.json`
6. **Link immediately** (update `.tsty/issues/{number}.json` with `linkedFlowId`)

**‚Üí Linking guide: [ISSUE-FLOW-LINKING.md](references/ISSUE-FLOW-LINKING.md)**

---

## üñ•Ô∏è Testing Development Servers

**Dev servers have expected errors** that should NOT fail tests:

```
Expected in dev mode:
‚úì HMR (Hot Module Reload) messages
‚úì 404s for source maps (*.map files)
‚úì React DevTools notifications
‚úì Fast Refresh rebuilding logs
‚úì Next.js compilation warnings
```

### When to Disable Console Monitoring

**Use `--no-monitor` flag for:**

1. **Local development servers** (default for tsty framework itself)
   ```bash
   tsty run my-flow --fail-fast --no-monitor
   ```

2. **Testing a framework that tests itself** (meta-testing)
   ```bash
   # When testing tsty's own dashboard
   tsty run test-dashboard-feature --no-monitor
   ```

3. **Any dev server with expected console noise**

### When to Enable Console Monitoring

**Use `monitorConsole: true` for:**

1. **Production builds in CI/CD**
   ```json
   {
     "monitorConsole": true,
     "failFast": true
   }
   ```

2. **Testing user-facing applications** (not dev infrastructure)

3. **Strict quality gates** where ANY console error should fail

### Configuration Examples

**Dev environment** (lenient):
```json
{
  "name": "Test Feature",
  "baseUrl": "http://localhost:3000",
  "monitorConsole": false,
  "failFast": true
}
```

**Production/CI** (strict):
```json
{
  "name": "Test Feature",
  "baseUrl": "https://app.example.com",
  "monitorConsole": true,
  "failFast": true
}
```

**Rule of thumb**: If testing shows 404s or HMR messages ‚Üí Use `--no-monitor`

---

## üîÑ Autonomous GitHub Issue Fixing

**CRITICAL**: When GitHub issues are mentioned, use **AUTONOMOUS WORKFLOW** - don't just track, actually FIX them.

### Autonomous Workflow (Primary)

**User says:** "Fix issue #42" or "Handle this GitHub issue"

**You autonomously:**

1. **Fetch & Understand**
   ```bash
   gh issue view 42 --repo owner/repo --json title,body,labels,number
   ```
   - Read issue description, labels, comments
   - Understand what's broken or missing

2. **Create Test Flow Automatically**
   - Analyze issue to identify affected feature
   - Write test flow that reproduces the bug
   - Save to `.tsty/flows/issue-42-<slug>.json`

3. **Run Test to Confirm Issue**
   ```bash
   tsty run issue-42-<slug> --fail-fast --no-monitor
   ```
   - Test should FAIL (confirming bug exists)
   - List and read ALL screenshots to understand the issue visually

4. **Analyze Failure & Fix Code**
   - List screenshots: `ls -1 .tsty/screenshots/<run-id>/*.png`
   - Read EVERY screenshot PNG file
   - Identify root cause from visual evidence
   - Use Edit tool to fix the code
   - Apply fix to relevant component/page files

5. **Verify Fix Works**
   ```bash
   tsty run issue-42-<slug> --fail-fast --no-monitor
   ```
   - Test should PASS (issue fixed!)
   - Compare before/after screenshots to verify visual improvement
   - If fails: iterate (analyze ‚Üí fix ‚Üí test) up to 3 times

6. **Close Issue on GitHub**
   ```bash
   gh issue close 42 --repo owner/repo --comment "Fixed! Visual verification confirms the improvement."
   ```
   - Include before/after description from screenshots
   - Reference test flow and screenshot paths

**‚Üí Complete guide: [AUTONOMOUS-ISSUE-FIXING.md](references/AUTONOMOUS-ISSUE-FIXING.md)**

### When to Use Autonomous Workflow

- ‚úÖ User says "fix issue #X" ‚Üí Full autonomous workflow
- ‚úÖ User says "handle this GitHub issue" ‚Üí Full autonomous workflow
- ‚úÖ User mentions bug from GitHub ‚Üí Fetch and offer to fix
- ‚ö†Ô∏è User says "test issue #X" ‚Üí Create test but ask before fixing
- ‚ùå User just wants to track issues ‚Üí Use manual workflow below

### Manual Workflow (Fallback)

For cases where autonomous fixing isn't appropriate (e.g., complex architectural changes):

1. Fetch: `gh issue view <number> --repo owner/repo --json title,body,labels,number`
2. Create test flow manually in `.tsty/flows/`
3. Run test and capture screenshots
4. Apply fixes based on visual analysis
5. Re-run and compare before/after screenshots
6. Close issue: `gh issue close <number> --repo owner/repo --comment "Fixed..."`

### Features

**Issue Management:**
- Fetch issues from any GitHub repo
- Store locally with tsty-specific metadata
- Link to test flows for automated verification
- Track testing timeline (fetch ‚Üí link ‚Üí reference ‚Üí latest)

**Reference Run System:**
- Mark any test run as a baseline
- Compare subsequent runs to baseline
- Stored in flow JSON and report JSON
- Clear reference when no longer needed

**Visual Comparison:**
- Side-by-side screenshots (before/after)
- Blue border for reference, green for current
- Step selector with changed step indicators
- Keyboard navigation (‚Üê ‚Üí arrows)
- "Full Screen" button for detailed comparison

**Use Cases:**
- Verify bug fixes visually
- Track feature changes over time
- Document issue resolution with screenshots
- Regression detection (did fix break something else?)

**Dashboard Pages:**
- `/issues` - List all fetched issues
- `/issues/<number>` - Issue detail with timeline and comparison
- `/compare` - Full-screen comparison view

---

## When to Read What

**Working with GitHub issues:**
- [AUTONOMOUS-ISSUE-FIXING.md](references/AUTONOMOUS-ISSUE-FIXING.md) - ‚≠ê **PRIMARY WORKFLOW** - Fetch ‚Üí Test ‚Üí Fix ‚Üí Verify

**Starting your first test:**
- [E2E-TESTING-GUIDE.md](references/E2E-TESTING-GUIDE.md) - HTML-first approach
- [USER-FIRST-TESTING.md](references/USER-FIRST-TESTING.md) - User-like interactions

**After EVERY test run:**
- [VERIFICATION-CHECKLIST.md](references/VERIFICATION-CHECKLIST.md) - Step-by-step validation

**When feature doesn't work:**
- [BUG-DETECTION-CHECKLIST.md](references/BUG-DETECTION-CHECKLIST.md) - ‚≠ê **START HERE** - Systematic bug finding
- [BUG-FIXING-WORKFLOW.md](references/BUG-FIXING-WORKFLOW.md) - Investigation & fixing

**Building complex flows:**
- [FLOW-STRUCTURE.md](references/FLOW-STRUCTURE.md) - JSON schema
- [EXAMPLES.md](references/EXAMPLES.md) - 11 real-world patterns

**Need reference:**
- [ACTIONS.md](references/ACTIONS.md) - 48 Playwright primitives
- [VARIABLES.md](references/VARIABLES.md) - Dynamic variables & Faker
- [CLI-REFERENCE.md](references/CLI-REFERENCE.md) - All commands
- [CONFIG.md](references/CONFIG.md) - Configuration options

**Troubleshooting:**
- [ANALYSIS-METHODS.md](references/ANALYSIS-METHODS.md) - Analyzing reports/screenshots
- [FAIL-FAST-GUIDE.md](references/FAIL-FAST-GUIDE.md) - Stopping on first failure

**Using dashboard:**
- [DASHBOARD.md](references/DASHBOARD.md) - Visual editors

---

## Essential CLI Commands

```bash
# Setup
tsty init                          # Create .tsty/ directory

# Running tests (ALWAYS use --fail-fast during development)
tsty run <flow> --fail-fast        # Stop on first failure (60-78% faster)
tsty run <flow> --device mobile    # Test on mobile viewport
tsty run <flow> --mark-reference   # Run and mark as reference baseline

# Listing
tsty list                          # List flows
tsty list actions                  # List user actions
tsty primitives                    # List 48 primitives
tsty primitives mouse              # List mouse primitives

# Screenshot Analysis
ls -1 .tsty/screenshots/<run-id>/*.png        # List all screenshots from a run
ls -1 .tsty/screenshots/run-*-<before>/*.png  # List before screenshots
ls -1 .tsty/screenshots/run-*-<after>/*.png   # List after screenshots

# GitHub Issue Integration (using gh CLI)
gh issue view <number> --repo owner/repo --json title,body,labels,number  # Fetch issue
gh issue list --repo owner/repo --limit 10                                # List issues
gh issue close <number> --repo owner/repo --comment "Fixed!"             # Close issue

# Dashboard
tsty                               # Start visual dashboard (localhost:4000)
```

**‚Üí Full reference: [CLI-REFERENCE.md](references/CLI-REFERENCE.md)**

---

## Prerequisites

**‚ö†Ô∏è CRITICAL: Before anything, initialize:**

```bash
tsty init  # Creates .tsty/ directory with config, subdirectories
```

**If you see "No configuration file found"** ‚Üí Run `tsty init` first.

---

## Workflow Approaches

**CLI-Only (Recommended for Autonomous Testing)**
- Create flows as JSON in `.tsty/flows/`
- Run via CLI: `tsty run flow-name --fail-fast`
- Fastest iteration for autonomous fixing
- **‚ö†Ô∏è CRITICAL: ALWAYS use headless mode (`headless: true`) for autonomous/agentic testing**
  - Prevents browser windows from appearing
  - Faster execution
  - Lower resource usage
  - Essential for background/automated workflows

**Dashboard (For Interactive Editing)**
- Start: `tsty` ‚Üí http://localhost:4000
- Visual flow builder with drag-and-drop
- Still iterate: run ‚Üí fix ‚Üí re-run
- Can use `headless: false` for manual debugging only

**Tsty tests ANY web application** - your apps, third-party sites, local servers, etc.

---

## Core Iteration Loop

**Phase 1: Setup & Run**
```bash
tsty run my-test --fail-fast
```

**Phase 2: Analysis (CRITICAL)**

**CRITICAL FIRST QUESTION: Did the outcome make sense?**

**Outcome Verification Checklist:**
- [ ] Does the screenshot show the expected UI change?
- [ ] Is the created file/data functional (not just exists)?
- [ ] Would a real user be satisfied with this result?
- [ ] Did EVERY step achieve its intended outcome?

**If ANY checkbox is unchecked ‚Üí Investigate and FIX the bug**

**Then analyze ALL data sources:**
1. **Report** (`.tsty/reports/`): `success`, `error`, `consoleErrors`
2. **Screenshots** (`.tsty/screenshots/run-*/`): Visual changes
3. **Console Logs** (`steps[].console`): JS errors
4. **Assertions** (`steps[].assertions`): Failed assertions

**‚Üí See [ANALYSIS-METHODS.md](references/ANALYSIS-METHODS.md) for complete guide.**

**Phase 3: Fix & Re-run (ITERATE)**

**Identify what to fix:**
- **Console errors** ‚Üí Fix **application code** (JS bugs in .tsx/.ts/.jsx/.js files)
- **Selector timeouts** ‚Üí Fix **test selectors** (wrong selector in flow/action JSON)
- **Failed assertions** ‚Üí Fix **test assertions** or **app behavior**
- **Visual bugs** ‚Üí Fix **application code** (CSS/layout in .css/.tsx files)
- **Missing elements** ‚Üí Fix **application code** (component rendering)

**Apply fixes:**
1. Use Read tool to examine relevant files
2. Use Edit tool to fix issues
3. Re-run: `tsty run my-test --fail-fast`
4. Repeat Phase 2 (analyze)
5. Continue until: exit 0, screenshots correct, no errors

**‚Üí See [ITERATIVE-WORKFLOW.md](references/ITERATIVE-WORKFLOW.md) for detailed patterns.**

---

## Decision Tree: STOP or CONTINUE?

**After EVERY test run:**

```
Exit code 0?
‚îú‚îÄ YES ‚Üí View screenshots ‚Üí Issues? ‚Üí Note as UX improvements ‚Üí DONE
‚îî‚îÄ NO ‚Üí consoleErrors > 0?
   ‚îú‚îÄ YES ‚Üí üö® FIX APP CODE (JS bug) ‚Üí Re-run
   ‚îî‚îÄ NO ‚Üí Selector timeout?
      ‚îú‚îÄ YES ‚Üí Check screenshot ‚Üí Fix selector or app ‚Üí Re-run
      ‚îî‚îÄ NO ‚Üí Read error ‚Üí Fix ‚Üí Re-run
```

**Key principles:**
- **Console errors** = Fix **app code** (JavaScript bugs)
- **Selector errors** = Fix **test selectors** (wrong selector)
- **Exit 0 + screenshots OK** = DONE (create next test)

**‚Üí See [QUICK-DECISIONS.md](references/QUICK-DECISIONS.md) for complete decision trees.**

---

## Flow Structure

**Minimal example:**

```json
{
  "name": "Test",
  "baseUrl": "http://localhost:3000",
  "failFast": true,
  "steps": [{
    "name": "Homepage",
    "url": "/",
    "capture": { "screenshot": true },
    "primitives": [{ "type": "click", "selector": "button" }]
  }]
}
```

**Key fields:** `url` (navigate), `primitives` (actions), `expectedUrl` (verify), `capture` (screenshots/HTML)

**‚Üí See [FLOW-STRUCTURE.md](references/FLOW-STRUCTURE.md) for complete schema.**

---

## 48 Playwright Primitives

**Primitives** are framework-provided building blocks (auto-generated from Playwright).
**Actions** are user-created behaviors (composed from primitives).

**48 primitives across 7 categories**: Navigation (5), Mouse (5), Input (8), Waiting (6), Locators (7), Info (4), Other (13)

```bash
tsty primitives           # List all 48 primitives
tsty primitives mouse     # List by category
```

**‚Üí See [ACTIONS.md](references/ACTIONS.md) for complete primitive reference and examples.**

---

## Variable Interpolation

**Syntax:** `${variable}`

**Built-in:** `${timestamp}`, `${datetime}`, `${baseUrl}`, `${credentials.email}`

**Faker.js (300+):**
```
${faker.person.fullName}
${faker.internet.email}
${faker.location.city}
${faker.company.name}
${faker.lorem.sentence}
```

**‚Üí See [VARIABLES.md](references/VARIABLES.md) for complete reference.**

---

## Configuration

`.tsty/config.json`:

```json
{
  "baseUrl": "http://localhost:3000",
  "credentials": { "email": "test@example.com", "password": "pass" },
  "playwright": { "headless": true, "timeout": 30000 }
}
```

**‚ö†Ô∏è CRITICAL for Autonomous/Agentic Testing:**
- **ALWAYS set `"headless": true`** (default, but verify!)
- Only use `"headless": false` for manual debugging with visible browser
- Headless mode prevents browser windows during autonomous iteration

**‚Üí See [CONFIG.md](references/CONFIG.md) for complete guide.**

---

## Troubleshooting

**Common issues:**
- "No configuration file" ‚Üí Run `tsty init`
- Port conflict ‚Üí `tsty --port 3001`
- Flow not found ‚Üí Check `tsty list` or `ls .tsty/flows/`
- Playwright not installed ‚Üí `npx playwright install chromium`
- Timeout errors ‚Üí Check selectors and page load state

**Note:** Playwright launches isolated browser instances. Dev server must be running before tests.

**‚Üí See [TROUBLESHOOTING.md](references/TROUBLESHOOTING.md) for complete guide.**

---

## Reference Documentation

**Load as needed for detailed information.**

### üéØ Core Workflows (Read First)

- **[ITERATIVE-WORKFLOW.md](references/ITERATIVE-WORKFLOW.md)** - ‚≠ê CRITICAL: Iteration loop with examples
- **[E2E-TESTING-GUIDE.md](references/E2E-TESTING-GUIDE.md)** - ‚≠ê CRITICAL: HTML-first approach
- **[VERIFICATION-CHECKLIST.md](references/VERIFICATION-CHECKLIST.md)** - ‚≠ê CRITICAL: Step-by-step validation
- **[ANALYSIS-METHODS.md](references/ANALYSIS-METHODS.md)** - Analyzing reports/screenshots/logs
- **[USER-FIRST-TESTING.md](references/USER-FIRST-TESTING.md)** - ‚≠ê User-like interactions & common mistakes
- **[BUG-FIXING-WORKFLOW.md](references/BUG-FIXING-WORKFLOW.md)** - ‚≠ê Investigation & bug fixing

### üìö Technical References

- **[FLOW-STRUCTURE.md](references/FLOW-STRUCTURE.md)** - Flow JSON schema with examples
- **[ACTIONS.md](references/ACTIONS.md)** - 48 Playwright primitives
- **[VARIABLES.md](references/VARIABLES.md)** - Variable interpolation & Faker.js
- **[CONFIG.md](references/CONFIG.md)** - Configuration options
- **[CLI-REFERENCE.md](references/CLI-REFERENCE.md)** - All CLI commands

### üé® Features & Guides

- **[DASHBOARD.md](references/DASHBOARD.md)** - Dashboard features
- **[FAIL-FAST-GUIDE.md](references/FAIL-FAST-GUIDE.md)** - Fail-fast mode details
- **[EXAMPLES.md](references/EXAMPLES.md)** - 11 real-world examples

### üìê Standards & Analysis

- **[VISUAL-ANALYSIS-GUIDE.md](references/VISUAL-ANALYSIS-GUIDE.md)** - Visual analysis methodology
- **[ANALYSIS-CHECKLIST.md](references/ANALYSIS-CHECKLIST.md)** - Quick checklist
- **[accessibility-standards.md](references/accessibility-standards.md)** - WCAG AA standards
- **[ux-production-analysis.md](references/ux-production-analysis.md)** - UX patterns
- **[framework-fixes.md](references/framework-fixes.md)** - Framework-specific fixes
- **[common-ux-issues.md](references/common-ux-issues.md)** - Common anti-patterns
- **[layout-patterns.md](references/layout-patterns.md)** - Layout best practices

### üõ†Ô∏è For Maintainers

- **[DESIGN-PHILOSOPHY.md](references/DESIGN-PHILOSOPHY.md)** - Design principles & extension guidelines

---

## When to Use This Skill

**Auto-trigger (proactive):**
- User implements UI features ‚Üí Test with Tsty
- User updates pages ‚Üí Verify no regressions
- User makes layout changes ‚Üí Check visual correctness

**Explicit requests:**
- "Test the dashboard visually"
- "Check accessibility"
- "Run E2E tests"
- "Find layout issues"

**Proactive suggestions:**
- User: "Added modal" ‚Üí You: "Let me test it"
- User: "Updated layout" ‚Üí You: "I'll run regression tests"

---

## Key Principles

### Technical
- **Directory**: `.tsty/`
- **Commands**: `tsty` or `npx qa` (never run scripts directly)
- **Primitives**: 48 auto-generated from Playwright's Page API
- **Actions**: User-created behaviors composed from primitives
- **Variables**: Faker.js for dynamic data (300+)
- **Screenshots**: `run-{flowId}-{timestamp}/` per run

### Workflow
- **ITERATE**: Run ‚Üí Analyze ‚Üí Fix ‚Üí Re-run (don't stop after one run)
- **ANALYZE ALL**: Reports + screenshots + logs + assertions
- **FIX AUTONOMOUSLY**: Apply fixes immediately
- **VERIFY FIXES**: Always re-run after fixes

### Analysis
- **Visible only**: Analyze what's in screenshots/logs
- **Priority**: Errors ‚Üí Critical bugs ‚Üí Assertions ‚Üí Visual ‚Üí UX
- **Two-tier**: Critical issues + UX improvements
- **Concise**: 1-2 lines per issue

---

## Framework Info

- **Package**: `tsty` (install from GitHub: `bun install -g https://github.com/mde-pach/tsty.git`)
- **GitHub**: https://github.com/mde-pach/tsty
- **Playwright**: https://playwright.dev
- **Faker.js**: https://fakerjs.dev/api/
